# ThaiElect69Raw Crawler

This directory contains a Python crawler and related artifacts for downloading and indexing election‑related files from the Election Commission of Thailand (ECT) website for the 2026 election.

## Files in this folder

- **crawler.py**  
  Command‑line script that:
  - Crawls the ECT election 2026 page (or a local HTML file) and looks for Google Drive folders and direct file links.
  - Downloads files (PDF, Office documents, images, archives, etc.) into a structured output directory (default: `ect_election_2026_downloads`).
  - Uses Selenium headless browser as a fallback when the site content is rendered dynamically with JavaScript.
  - Supports downloading from Google Drive either via:
    - The official Google Drive API using a service account, or
    - `gdown` as a fallback for public folders.

- **crawler.log**  
  Log file generated by `crawler.py`. It records:
  - URLs being accessed.
  - Links and Google Drive folders detected.
  - Download attempts, successes, failures, and summary statistics.
  - Any warnings or errors from the crawler or the local development server.

- **files_collected.csv**  
  CSV index of downloaded files (created by a separate processing step) with columns such as:
  - `relative_path` – Path relative to the root download directory.
  - `filename`, `extension`.
  - `size_bytes`, `size_mb`.
  - `modified_date`.
  - `province`, `district`, `subdistrict`, `depth` – Location hierarchy inferred from the folder structure (Thai names).
  - `md5_hash` – File checksum (if calculated; `N/A` otherwise).
  - `full_path` – Absolute file path on disk.

## Requirements

`crawler.py` relies on:

- Python 3.9+ (recommended).
- Python packages:
  - `requests`
  - `beautifulsoup4`
  - `selenium`
  - `webdriver-manager`
  - `gdown`
  - `python-dotenv`
  - `google-api-python-client`
  - `google-auth`

Install them, for example:

```bash
pip install requests beautifulsoup4 selenium webdriver-manager gdown python-dotenv google-api-python-client google-auth
```

You also need a recent Chrome or Edge browser installed locally so Selenium can run a headless browser.

## Google Drive API configuration (optional but recommended)

To let `crawler.py` use the official Google Drive API instead of only `gdown`, set an environment variable `GOOGLE_SERVICE_ACCOUNT_JSON` containing the full JSON of a Google service account with **Drive read‑only** access.

The script loads it via `python-dotenv`, so you can store it in a local `.env` file:

```env
GOOGLE_SERVICE_ACCOUNT_JSON={"type": "...", "project_id": "...", ...}
```

If this variable is not set or the service account cannot access a folder, the crawler falls back to `gdown` for public folders.

## How to run the crawler

From this directory:

```bash
python crawler.py --url "https://www.ect.go.th/ect_th/th/election-2026" --output "ect_election_2026_downloads"
```

Useful options:

- `--url`  
  - Target URL to crawl (ECT page, Google Drive folder URL, or path to a local HTML file, e.g. `raw_element.txt`).

- `--output`  
  - Directory where downloads will be stored (default: `ect_election_2026_downloads`).

- `--ext`  
  - One or more file extensions to download, e.g.:

    ```bash
    python crawler.py --url "https://www.ect.go.th/ect_th/th/election-2026" --ext .pdf .xlsx
    ```

- `--dry-run`  
  - Scan and log what would be downloaded without actually saving any files.

- `--force`  
  - Re‑download content even if a province folder already has a `.completed` marker file.

- `--workers`  
  - Number of parallel threads for downloading Google Drive folders (default: 4).

Example using a local HTML snapshot (for when the live site changes or is unstable):

```bash
python crawler.py --url "raw_element.txt" --output "ect_election_2026_downloads" --dry-run
```

## Interpreting files_collected.csv

Once the downloads exist under `ect_election_2026_downloads`, `files_collected.csv` provides a convenient index for analysis. Typical uses:

- Filter by province / district / subdistrict to locate specific PDFs.
- Sort by `modified_date` or `size_mb` to find the newest or largest files.
- Use `md5_hash` (when present) to detect duplicates across locations.

The CSV is encoded in UTF‑8 and uses Thai names for geographic levels, matching the folder hierarchy of the downloaded files.

## Logs and troubleshooting

- Check `crawler.log` to see:
  - When the crawler last ran and which URL it accessed.
  - Whether Google Drive links were found and how many.
  - Any download errors or API authentication problems.
- If the ECT site loads content dynamically and no links are found:
  - The script automatically tries a Selenium headless browser.
  - As a last resort, you can save the relevant HTML section to a local file (e.g. `raw_element.txt`) and point `--url` to that file.


Here is an overview analysis of files_collected.csv :

Summary

- จำนวนแถว (ไฟล์ทั้งหมด): 109,763 แถว
- ขนาดรวม (bytes): 314,341,608,638 ≈ 300,000 MB (ประมาณ 300 TB ตาม field รวม; เป็นการรวมจาก size_mb ทุกแถว)
- จำนวนแถวที่มี md5_hash จริง (ไม่ใช่ N/A ): 0
  - ตอนนี้ column md5_hash ยังว่างสำหรับทุกไฟล์ (ใช้เป็น placeholder เฉย ๆ)
ชนิดไฟล์ (extension)

Top 10 ตามจำนวนไฟล์:

- .pdf : 92,262 ไฟล์ (ส่วนใหญ่เป็นเอกสารเขต/หน่วยเลือกตั้ง)
- .jpg : 14,683
- .jpeg : 1,865
- .ini : 365
- .png : 72
- (empty) : 68 (ไม่มีนามสกุล)
- .download : 57 (ไฟล์ดาวน์โหลดไม่สมบูรณ์)
- .heic : 56
- .db : 52
- .zip : 30
สรุป: ข้อมูลหลักเป็น ไฟล์ PDF จำนวนมาก และตามด้วยรูปภาพ (.jpg/.jpeg)

จังหวัดที่มีไฟล์เยอะที่สุด (ตามจำนวนไฟล์)

Top 10 provinces by file count:

1. นครราชสีมา – 7,867 ไฟล์
2. อุบลราชธานี – 5,237
3. ร้อยเอ็ด – 5,214
4. สุรินทร์ – 4,718
5. ปราจีนบุรี – 4,360
6. เชียงใหม่ – 4,258
7. สมุทรปราการ – 4,212
8. สงขลา – 3,409
9. มหาสารคาม – 3,083
10. นครศรีธรรมราช – 3,059
แปลว่าไฟล์แบ่งตามจังหวัดครบและบางจังหวัดมีไฟล์จำนวนมาก (อาจมีหลายหน่วย/หลายประเภทเอกสาร)

จังหวัดที่ใช้พื้นที่เยอะที่สุด (ตามขนาดรวม bytes)

Top 10 provinces by total size:

1. นครราชสีมา – 21,296,804,328 bytes
2. สุรินทร์ – 15,788,721,390
3. นครปฐม – 15,546,639,203
4. สมุทรปราการ – 15,281,795,634
5. มหาสารคาม – 11,915,294,648
6. กรุงเทพมหานคร – 10,989,437,651
7. อำนาจเจริญ – 10,407,246,041
8. อุบลราชธานี – 10,332,018,659
9. ร้อยเอ็ด – 9,799,807,256
10. เชียงใหม่ – 9,089,306,976
บางจังหวัด (เช่น นครปฐม, อำนาจเจริญ, กรุงเทพมหานคร) อาจมีไฟล์ขนาดใหญ่มาก แม้จำนวนไฟล์จะไม่สูงสุด

โครงสร้างลึกของโฟลเดอร์ ( depth )

ค่า depth มาจากจำนวนระดับใน relative_path (เช่น จังหวัด → เขต → อำเภอ → ตำบล → ฯลฯ):

- depth = 6 : 70,418 แถว
- depth = 7 : 21,198
- depth = 5 : 11,319
- depth = 8 : 3,626
- depth = 4 : 2,270
- depth = 9 : 794
- depth = 3 : 112
- depth = 10 : 26
ส่วนใหญ่ไฟล์อยู่ที่ความลึก 5–7 ระดับ ซึ่งสอดคล้องกับโครงสร้าง จังหวัด → เขต → อำเภอ/ประเภท → ตำบล/เทศบาล → รายละเอียดหน่วยเลือกตั้ง เป็นต้น

# ThaiElect69Raw Crawler

This directory contains a Python crawler and related artifacts for downloading and indexing election‑related files from the Election Commission of Thailand (ECT) website for the 2026 election.

## Files in this folder

- **crawler.py**  
  Command‑line script that:
  - Crawls the ECT election 2026 page (or a local HTML file) and looks for Google Drive folders and direct file links.
  - Downloads files (PDF, Office documents, images, archives, etc.) into a structured output directory (default: `ect_election_2026_downloads`).
  - Uses Selenium headless browser as a fallback when the site content is rendered dynamically with JavaScript.
  - Supports downloading from Google Drive either via:
    - The official Google Drive API using a service account, or
    - `gdown` as a fallback for public folders.

- **crawler.log**  
  Log file generated by `crawler.py`. It records:
  - URLs being accessed.
  - Links and Google Drive folders detected.
  - Download attempts, successes, failures, and summary statistics.
  - Any warnings or errors from the crawler or the local development server.

- **files_collected.csv**  
  CSV index of downloaded files (created by a separate processing step) with columns such as:
  - `relative_path` – Path relative to the root download directory.
  - `filename`, `extension`.
  - `size_bytes`, `size_mb`.
  - `modified_date`.
  - `province`, `district`, `subdistrict`, `depth` – Location hierarchy inferred from the folder structure (Thai names).
  - `md5_hash` – File checksum (if calculated; `N/A` otherwise).
  - `full_path` – Absolute file path on disk.

## Requirements

`crawler.py` relies on:

- Python 3.9+ (recommended).
- Python packages:
  - `requests`
  - `beautifulsoup4`
  - `selenium`
  - `webdriver-manager`
  - `gdown`
  - `python-dotenv`
  - `google-api-python-client`
  - `google-auth`

Install them, for example:

```bash
pip install requests beautifulsoup4 selenium webdriver-manager gdown python-dotenv google-api-python-client google-auth
```

You also need a recent Chrome or Edge browser installed locally so Selenium can run a headless browser.

## Google Drive API configuration (optional but recommended)

To let `crawler.py` use the official Google Drive API instead of only `gdown`, set an environment variable `GOOGLE_SERVICE_ACCOUNT_JSON` containing the full JSON of a Google service account with **Drive read‑only** access.

The script loads it via `python-dotenv`, so you can store it in a local `.env` file:

```env
GOOGLE_SERVICE_ACCOUNT_JSON={"type": "...", "project_id": "...", ...}
```

If this variable is not set or the service account cannot access a folder, the crawler falls back to `gdown` for public folders.

## How to run the crawler

From this directory:

```bash
python crawler.py --url "https://www.ect.go.th/ect_th/th/election-2026" --output "ect_election_2026_downloads"
```

Useful options:

- `--url`  
  - Target URL to crawl (ECT page, Google Drive folder URL, or path to a local HTML file, e.g. `raw_element.txt`).

- `--output`  
  - Directory where downloads will be stored (default: `ect_election_2026_downloads`).

- `--ext`  
  - One or more file extensions to download, e.g.:

    ```bash
    python crawler.py --url "https://www.ect.go.th/ect_th/th/election-2026" --ext .pdf .xlsx
    ```

- `--dry-run`  
  - Scan and log what would be downloaded without actually saving any files.

- `--force`  
  - Re‑download content even if a province folder already has a `.completed` marker file.

- `--workers`  
  - Number of parallel threads for downloading Google Drive folders (default: 4).

Example using a local HTML snapshot (for when the live site changes or is unstable):

```bash
python crawler.py --url "raw_element.txt" --output "ect_election_2026_downloads" --dry-run
```

## Interpreting files_collected.csv

Once the downloads exist under `ect_election_2026_downloads`, `files_collected.csv` provides a convenient index for analysis. Typical uses:

- Filter by province / district / subdistrict to locate specific PDFs.
- Sort by `modified_date` or `size_mb` to find the newest or largest files.
- Use `md5_hash` (when present) to detect duplicates across locations.

The CSV is encoded in UTF‑8 and uses Thai names for geographic levels, matching the folder hierarchy of the downloaded files.

## Logs and troubleshooting

- Check `crawler.log` to see:
  - When the crawler last ran and which URL it accessed.
  - Whether Google Drive links were found and how many.
  - Any download errors or API authentication problems.
- If the ECT site loads content dynamically and no links are found:
  - The script automatically tries a Selenium headless browser.
  - As a last resort, you can save the relevant HTML section to a local file (e.g. `raw_element.txt`) and point `--url` to that file.

